# 인공지능 미션 학습내용 정리 I

## Numpy 사용하기

- **Numpy**와 **Scipy**는 Python 언어에서 사용되는 라이브러리이다.
- **Numpy**는 Python 언어에서 기본으로 지원하지 않는 **array (배열)** 혹은 **matrix (행렬)** 의 계산을 쉽게 할 수 있도록 도와준다.
- 이 외에도 많은 **수학 함수**들을 포함하고 있어 Numpy를 사용하면 기계학습에서 많이 사용되는 **해석학 및 선형대수학**에 관련된 수식들을 Python 위에서 쉽게 프로그래밍 할 수 있다.

### numpy.ndarray

$$
\left(\begin{array}{cc} 
1 & 2\\
3 & 4
\end{array}\right)
$$

- 위와 같은  `2×2` 배열(또는 행렬) 을 만들기 위해서 다음과 같이 작성할 수 있다. `ndarray`를 만들기 위해서 `array()` 명령어를 사용한다.

  ```
  numpy.array([[1, 2],
  			 [3, 4]])
  ```

### numpy.ndarray.shape

- Numpy의 배열에서 **shape**는 **배열의 모양**을 의미한다. 위의 행렬의 shape는 (2,2)의 모양을 가지고 있다.

- `Numpy.array` 배열의 모양을 확인하려면 다음과 같은 코드를 사용할 수 있다.

  ```python
  A = numpy.array([[1, 2, 3], [4, 5, 6]])
  print(A.shape)  # (2, 3)
  ```

### numpy.ndarray.reshape

- Numpy를 사용하여 배열의 모양을 원하는 대로 바꿀 수 있다. 2×4 배열을 4×2 또는 1×8 과 같은 모양으로 원하는 대로 바꿀 수 있다. 다만, 이때 바뀐 모양에 있는 원소의 수가 같지 않다면 (이를테면 2×2 에서 4×4로) 에러가 발생한다.

- 우선 `Numpy.array` 배열의 모양을 확인하려면 다음과 같은 코드를 사용할 수 있다.

  ```python
  A = numpy.array([[1, 2, 3], [4, 5, 6]])
  print(A.shape)  # (2, 3)
  ```

- 그리고 다음 명령어를 실행하면서 numpy가 배열 내의 원소의 순서를 어떻게 지키면서 모양을 바꾸는지 테스트해보자.

  ```python
  A = numpy.array([[1, 2, 3], [4, 5, 6]])
  print(A)  
  '''
  [[1 2 3]
   [4 5 6]]
  '''
  B = A.reshape((3, 2))
  print(B)
  '''
  [[1 2]
   [3 4]
   [5 6]]
  '''
  ```

### numpy.concatenate

- Numpy를 통해 여러 개의 배열을 한 개로 쉽게 합칠 수 있다. 합치는 방법에는 두 가지가 있는데 **X축 방향**으로 합치는 것과 **Y축 방향**으로 합치는 것이다. 이것은 `numpy.concatenate`에 들어가는 `axis` 라는 파라미터를 통해 제어할 수 있다.

  - **`axis = 0`**: **Y축 (세로 방향)** 으로 설정한다.

  - **`axis = 1`**: **X축 (가로 방향)**으로 설정한다.

    ```python
    A = numpy.array([[1, 2], [3, 4]])
    B = numpy.array([[5, 6], [7, 8]])
    
    C_Y = numpy.concatenate((A, B), axis = 0)
    print(C_Y)
    '''
    [[1 2]
     [3 4]
     [5 6]
     [7 8]]
    '''
    
    C_X = numpy.concatenate((A, B), axis = 1)
    print(C_X)
    '''
    [[1 2 5 6]
     [3 4 7 8]]
    '''
    ```

  ### numpy.split

- `numpy.split`은 배열을 여러 개의 크기로 나누어준다. 나누는 방법은 **X축**, 그리고 **Y축**을 기준으로 나누는 두 가지의 방법이 있으며, 위에서 본**`numpy.concatenate`의 `axis`와 동일**하게 작동한다. 또한 **두 번째 파라미터에**

  - **숫자 `N`**을 넣으면 **배열을 `N`개 동일한 크기의 배열들로 나누고**

  - **리스트**를 넣으면 **리스트 안의 숫자들 번째 인덱스에서 배열을 나눈다**. 예를 들어 [2, 3]이 입력될 경우 다음과 같이 나뉘게 된다.

    ```python
     0   1   2   3   4   5   6 ...
    [  X   X | X | X   X   X   ...
             ^   ^
             split!
    ```

    ```python
    A = numpy.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13 ,14 ,15, 16]])
    print(A)
    '''
    [[ 1  2  3  4]
     [ 5  6  7  8]
     [ 9 10 11 12]
     [13 14 15 16]]
    '''
    
    slice_Y_equal_size = numpy.split(A, 2, axis = 0)
    print(slice_Y_equal_size)
    '''
    [array([[1, 2, 3, 4],
           [5, 6, 7, 8]]), 
     array([[ 9, 10, 11, 12],
           [13, 14, 15, 16]])]
    '''
    
    print(slice_Y_equal_size[0])
    '''
    [[1 2 3 4]
     [5 6 7 8]]
    '''
    
    print(slice_Y_equal_size[1])
    '''
    [[ 9 10 11 12]
     [13 14 15 16]]
    '''
    
    #---------------------------------------------------
    print(A)
    '''
    [[ 1  2  3  4]
     [ 5  6  7  8]
     [ 9 10 11 12]
     [13 14 15 16]]
    '''
    slice_X_different_sizes = numpy.split(A, [2, 3], axis = 1)
    print(slice_X_different_sizes)
    '''
    [array([[ 1,  2],
           [ 5,  6],
           [ 9, 10],
           [13, 14]]), 
     array([[ 3],
           [ 7],
           [11],
           [15]]), 
     array([[ 4],
           [ 8],
           [12],
           [16]])]
    '''
    
    print(slice_X_different_sizes[0])
    '''
    [[ 1  2]
     [ 5  6]
     [ 9 10]
     [13 14]]
    '''
    
    print(slice_X_different_sizes[1])
    '''
    [[ 3]
     [ 7]
     [11]
     [15]]
    '''
    
    print(slice_X_different_sizes[2])
    '''
    [[ 4]
     [ 8]
     [12]
     [16]]
    '''
    ```

### numpy.ndarray.transpose

- Transpose는 전치행렬을 구하는 메소드이다. **배열의 (*i*, *j*) 번째 원소를 (*j*, *i*)번째 원소로 바꾼 뒤 리턴**한다

  ```python
  A = numpy.array([[1, 2, 3], [4, 5, 6]])
  print(A)
  '''
  [[1 2 3]
   [4 5 6]]
  '''
  print(A.transpose())
  '''
  [[1 4]
   [2 5]
   [3 6]]
  '''
  ```

### numpy.linalg.inv

- **행렬의 역행렬 (inverse)** 를 구할 때 쓸 수 있다.

  ```python
  A = numpy.array([[1, 2], [3, 4]])
  print(numpy.linalg.inv(A))
  '''
  [[-2.   1. ]
   [ 1.5 -0.5]]
  '''
  ```

### numpy.dot

- **두 행렬의 곱셈**, 혹은 **두 벡터의 dot product (내적)을 구하는 데에 사용**한다.

  ```python
  A = numpy.array([[1, 2, 3], [1, 2, 1]])
  '''
  [[1 2 3]
   [1 2 1]]
  '''
  B = numpy.array([[2, 1, 3], [-1, 0, 5]])
  C = numpy.dot(A, B) # Error!
  
  B = B.transpose()
  '''
  [[2 -1]
   [1 0]
   [3 5]]
  '''
  C = numpy.dot(A, B)
  print(C)
  '''
  [[13 14]
   [ 7  4]]
  '''
  ```

## 선형 회귀법

- **Linear Regression (선형회귀법)** 은 **스칼라로 표현되는 종속변수 y**와 **벡터 형식으로 표현되는 독립변수 X 간의 관계**를 표현하기 위한 알고리즘을 나타낸다.
- **독립변수 X가 스칼라 값일 경우 (즉 1차원 벡터일 경우)** 는 특별히 **단순회귀분석 (simple linear regression)** 이라고 부른다.
- 단순회귀분석에서 데이터는 **X와 y의 쌍의 집합**으로 표현되며, 독립변수 X가 스칼라값이기 때문에 데이터는 **2차원 평면**에 표시할 수 있다.
- **회귀분석**은 관찰된 변수들에 대해 **독립변수와 종속변수 사이의 관계**를 나타내는 선형 관계식을 구하는 알고리즘이다. 다시 말하면, 회귀분석은 **독립변수가 바뀜에 따라 종속변수가 어떻게 변하는지를 분석하는 것**이다. 그러므로 **시간과 관계된 데이터, 예측,** 그리고 **인과관계**를 분석하는데에 자주 사용된다.

### statsmodels.api.OLS

- Python 라이브러리 `statsmodels` 내에 있는 알고리즘으로, 다른 라이브러리와는 달리 **summary**를 제공해 훨씬 분석하기 쉽도록 정보를 표시해준다. 이 라이브러리는 데이터를 입력받은 뒤 회귀분석을 진행한다. 이 함수는 두 개의 변수 `x` 와 `y` 를 입력받는다.

  ```python
  import statsmodels.api
  import numpy
  
  X = [1, 2, 3, 4, 5]  # 1
  Y = [2, 3.7, 3.9, 5, 7]
  X = numpy.array(X).T  # 2
  Y = statsmodels.api.add_constant(X)  # 3
  results = statsmodels.api.OLS(Y, X).fit()
  
  '''
  #1) X는 선언 후 다음과 같은 일반적인 Python list가 된다.
  [1, 2, 3, 4, 5]
  
  #2) X에 Transpose를 취하면, X는 5×1 배열이 된다. 
  그 뒤, 상수를 더하는데, 상수를 더하는 이유는 선형회귀분석을 시행할 때 정확도를 올리기 위해서다.
  
  #3) 만약, y = 2x + 1의 관계를 x와 y가 가지고 있다면, 상수 없이 y = 2x 모델링하기에는 정확도가 떨어진다. 상수를 더한 후의 X의 모양은 다음과 같다.
  => [[ 1.  1.]
       [ 1.  2.]
       [ 1.  3.]
       [ 1.  4.]
       [ 1.  5.]]
  '''
  ```

### 선형회귀 실행 예시

```python
import numpy
import statsmodels.api
from draw_graph import visualize


def main():
    (N, X, Y) = read_data()
    results = do_simple_regression(N, X, Y)

    visualize(X, Y, results)

def read_data():
    # 1
    N = int(input())
    X, Y = [], []
    for _ in range(N):
        x, y = map(float, input().split())
        X.append(x)
        Y.append(y)
    return (N, X, Y)

def do_simple_regression(N, X, Y):
    # 2
    X = numpy.array(X).T
    X = statsmodels.api.add_constant(X)
    results = statsmodles.api.OLS(Y, X).fit()
    return results

if __name__ == "__main__":
    main()

'''
테스트 입력
5
1 1
2 4
3 9
4 16
5 25
'''
'''
테스트 출력
우상향 2차원 선형회귀 그래프가 나타남
'''

'''
main 함수의 하단부에 print(results.summary())을 넣어주면,
아래와 같은 결과가 나타난다.

                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.963
Model:                            OLS   Adj. R-squared:                  0.950
Method:                 Least Squares   F-statistic:                     77.14
Date:                Mon, 22 Feb 2021   Prob (F-statistic):            0.00311
Time:                        03:17:03   Log-Likelihood:                -9.6687
No. Observations:                   5   AIC:                             23.34
Df Residuals:                       3   BIC:                             22.56
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         -7.0000      2.266     -3.090      0.054     -14.210       0.210
x1             6.0000      0.683      8.783      0.003       3.826       8.174
==============================================================================
Omnibus:                          nan   Durbin-Watson:                   1.429
Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.674
Skew:                           0.256   Prob(JB):                        0.714
Kurtosis:                       1.276   Cond. No.                         8.37
==============================================================================

'''
```

#### 결과 해석하기

- 회귀분석을 수행하고 나면 각 독립변수에 대한 **계수(coefficient)** 및 **P-value 값**을 확인할 수 있다. 이 두 가지의 값을 통해 각 변수들이 종속변수에 어떻게 관계되어있는지를 확인할 수 있다.

  - **계수 (coefficient, coef)**: 만약 **계수가 양**이라면 해당하는 변수가 종속변수에 **양으로 연관**되어 있으며 **(positively correlated)**, **음이라면** 해당 변수가 종속변수에 **음으로 연관**되어 있음을 나타낸다. 예를 들어, 일반적으로 `섭취하는 음식의 칼로리` 라는 독립변수는 `비만율` 라는 종속변수에 양으로 연관되어 있다고 예측할 수 있다. 반대로, `운동 시간` 이라는 독립변수는 `비만율` 에 음으로 연관되어 있다고 생각할 수 있다.

  - **계수**는 다음과 같이 확인할 수 있다.

    ```python
    results.params
    ```

  - **P-value**: 각 **독립변수가 얼마나 종속변수에 영향을 미치는지**를 나타낸다. 정확히 말하면, 회귀분석에서 수행하는 테스트에서 **P값은 독립변수의 계수가 0일 확률**을 나타낸다. 즉, **P값이 작을수록 해당 독립변수가 모델에서 의미를 가지며, P값이 높을수록 해당 독립변수는 종속 변수에 영향을 끼치지 못하게 된다.** 일반적으로, **P값이 0.05 미만일 때 통계적으로 유의하다**고 한다. P-value 값을 확인하는 방법은 다음과 같다.

    ```python
    results.pvalues
    ```

- 위의 `OLS Regression Results`를 살펴보면, 변수 두 개 (const, x1)에 대해 분석이 이루어졌음을 알 수 있다. 

  - **계수 (Coef.)**: `const`, `x1`의 계수는 각각 -7.0, 6.0 이다. **`const`는 종속변수와 음의 상관관계**를 가지고 있고, **`x1`은 양의 상관관계**를 가지고 있다고 해석할 수 있다.
  - **P값 (P>|t|)**: `x1` 만이 0.05 미만의 값을 가지고 있으므로, 근소한 차이로 `const`는 종속변수와 유의미한 상관관계를 가지고 있지 않다고 보여진다. (`const`의 P값은 `0.0054`)

## 확률과 나이브 베이즈 미션

- **확률**: 확률 및 통계학에서 **어떤 사건(event) 이 일어날 것인지에 대한 정도 혹은 믿음의 측도(measure)** 이다. 확률은 숫자 **0부터 1**의 값으로 표시된다. 0은 이 사건이 결코 일어날 수 없음을(impossibility) 나타내고, 1은 사건이 확실히 일어남을(certainty) 의미한다. 

- **사건 A에 대한 확률**은 **P(A), Pr(A)**, 또는 **p(A)**로 표시된다. 사건 A 가 일어나지 않을 **여사건의 확률(*opposite* 또는 *compliment*)는 1 - P(A)**와 같다. 예를 들어, 완벽한 주사위가 있다고 할 때 주사위를 한 번 굴려서 1이 나오는 사건에 대한 확률은 
  $$
  \frac{1}{6}
  $$
  이다. 그리고 1이 나오는 사건의 여사건, 즉 1이 나오지 않는 사건에 대한 확률은 
  $$
  1-\frac{1}{6} = \frac{5}{6}
  $$
  이다.

- **사건 A와 B**가 한 번의 실험에서 **동시에 발생할 확률**은 ***P*(*A*∩*B*)** 로 표현된다. 예를 들어, 주사위를 던져 1-6의 숫자 중 하나를 뽑았을 때

  - **A**: 뽑은 숫자가 **2의 배수**일 확률
  - **B**: 뽑은 숫자가 **3의 배수**일 확률

  일 경우  

$$
P(A∩B) = \frac{1}{6}
$$

	이다. 

- 이와 달리, **사건 A 또는 B**가 발생할 확률, 즉 뽑은 숫자가 **2의 배수 또는 3의 배수**일 확률은 
  $$
  P(A∪B)= \frac{4}{6}
  $$
   이다. 지금부터 확률의 성질을 간단하게 알아보자.

- **독립 사건 (independent events)**: 두 사건 **A, B가 독립적**일 경우 **A, B의 결합 확률(joint probability)**은 
  $$
  P(A∩B)=P(A)P(B)
  $$
  이며, 예를 들어 동전 두 개를 던졌을 때 두 개 모두 앞이 나올 확률은 
  $$
  \frac{1}{2} \times \frac{1}{2} = \frac{1}{4}
  $$
  이다.

- **상호 배타적 사건 (mutually exclusive events)**: 만약 두 사건 **A, B 가 상호 배타적**인 경우, 즉 **A와 B가 동시에 일어날 수 없는 경우**, 
  $$
  P(A \cap B) = 0
  $$
  이며, 
  $$
  P(A \cup B) = P(A) + P(B)
  $$
  이다.

- **상호 배타적이지 않은 사건 (not mutually exclusive events)**: 만약 두 사건 **A, B가 같이 일어날 수 있는 경우** 
  $$
  P(A \cup B) = P(A) + P(B) - P(A \cap B)
  $$
  이다.

### 조건부확률 (conditional probability)

- **조건부확률**은 **B 사건이 일어나는 경우의 A 사건이 일어날 확률**이다. 조건부확률은 **P(A|B)**로 쓰여지며, “B 가 주어졌을 때 A 의 확률” 의 의미를 가진다. 이것은 다음과 같이 정의된다.

$$
P(A|B) = \frac{P(A \cap B)}{P(B)}
$$

- **P(B)가 0일 경우 조건부 확률은 성립하지 않는다**. 예를 들어, 위의 예제를 다시 생각해 보면, 주사위를 던져 1-6의 숫자 중 하나를 뽑았을 때

  - **A**: 뽑은 숫자가 **2의 배수**일 확률
  - **B**: 뽑은 숫자가 **3의 배수**일 확률

- **P(A|B)**는 **이미 뽑은 숫자가 3의 배수일 경우 그 중에 뽑은 숫자가 2의 배수일 확률**이다. 즉, 뽑은 숫자가 {3, 6} 둘 중의 하나일 때 숫자가 2의 배수일 확률이며, 이는
  $$
  \frac{1}{2}
  $$
  가 된다. 이것을 조건부 확률 공식에 다시 적용해 보면,

$$
P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{\frac{1}{6}}{\frac{2}{6}} = \frac{1}{2}
$$

		이다.

---

### 기계학습, 정보 검색, 데이터 마이닝

- **기계 학습(Machine Learning), 정보 검색(Information Retrieval), 그리고 데이터 마이닝(Data Mining)** 은 서로 비슷해 보이지만 약간씩 다른 의미를 가진다. 
- **기계 학습**은 데이터에서 **의미있는 패턴**을 찾아내는 *알고리즘* 을 의미하며, 이 알고리즘들은 정보 검색이나 데이터 마이닝에서 사용될 수도 있다. 
- **정보 검색**은 정보의 집합에서 **원하는 정보**를 뽑아내는 기술을 의미하며, 여러 정의가 있으나 **데이터의 *일부분*** 을 빠르게 찾아내는 기술을 의미한다. 
- **데이터 마이닝**은 **데이터 내부에 *숨겨진***  것을 찾아내는 기술을 의미한다.

### 자연어 처리

- 기계학습 알고리즘을 이용한 정보 검색 및 데이터 마이닝 작업에서는 자연어 텍스트의 표현을 위해 여러가지 방식을 사용한다. 그 모델들 중 하나로 **Bag-of-Words** **모델**이 있다.

- **Bag-of-Words 모델**은 **텍스트를 단어의 집합으로 표현하는 것**이다. 자연어 텍스트는 단어별로 쪼개져 섞인 뒤에 같은 단어끼리 뭉치게 된다. 이 과정에서, 단어의 순서나 문장/문법 구조는 사라지며 **최종적으로 단어 및 단어의 개수로 표현**되게 된다. Bag-of-Words는 문서 및 텍스트를 단어로 쪼개 한 가방 (Bag) 에 넣고 흔들어 섞은 뒤에 단어를 재정렬하는것과 비슷한 개념으로 설명할 수 있다.

## 그외 참고

- **몬테 카를로 방법(Monte Carlo method)**: 몬테 카를로 방법은 **임의의 샘플링(실험을 수행하는 것)을 계속 반복하여 특정 함수의 값을 알아내는 방법**을 의미한다. 예를 들어, 우리가 동전의 앞면이 나올 정확한 확률은 모르고 있지만 동전을 계속해서 던지게 되면 동전의 앞면이 나올 정확한 확률에 점점 더 가깝게 알 수 있다.

- **정서 분석(sentiment analysis)**: 감정을 크게 **긍정적인 감정 또는 부정적인 감정으로 나누는 문제**를 말한다.

- **감정 분석(emotion analysis)**: 이 외에 사람들이 느끼는 감정들, 예를 들어 **분노/기쁨/놀람/사랑/슬픔/두려움(Parrott’s Emotions)** 혹은 **기쁨/신뢰/두려움/놀람/슬픔/역겨움/화남/기대(Plutchik’s Wheel of Emotions)** 등의 분석을 하는 문제를 말한다.

  

<br/>

---

※ 엘리스가 제공한 학습 자료, 콘텐츠의 저작권은 엘리스에 있다. <br>

**※ COPYRIGHT 2016-2021. ELICE ALL RIGHTS RESERVED.**